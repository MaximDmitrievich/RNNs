{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from matplotlib import pylab\n",
    "from sklearn.manifold import TSNE\n",
    "import operator # sorting items in dictionary by value\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные\n",
    "\n",
    "В файле pets.txt хранится файл со статьей из википедии про животных, почитать ее можете [здесь](https://en.wikipedia.org/wiki/Pet). В .txt файле текст предобработан, извлечены все ссылки. Мы будем делать препроцессинг этого текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.',' ')\n",
    "    text = text.replace(',',' ')\n",
    "    text = text.replace('-',' ')\n",
    "    text = text.replace('%','')\n",
    "    text = text.replace(':',' ')\n",
    "    text = text.replace('(','')\n",
    "    text = text.replace(')','')\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = text.replace('[',' ')\n",
    "    text = text.replace(']',' ')\n",
    "    text = text.replace('\"',' ')\n",
    "    text = text.replace('  ',' ')\n",
    "    \n",
    "    return text\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "\n",
    "    with open(filename,'r') as f:\n",
    "        data = []\n",
    "        file_string = f.read()\n",
    "        file_string = preprocess_text(file_string)\n",
    "        file_string = file_string.split(' ')\n",
    "        data.extend(file_string)\n",
    "    return data\n",
    "words = read_data('pets.txt')\n",
    "print('Data size %d' % len(words))\n",
    "print('Example words (start): ',words[:10])\n",
    "print('Example words (end): ',words[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 0\n",
    "def build_dataset(words):\n",
    "    global vocabulary_size\n",
    "\n",
    "    count = [['UNK', -1]]\n",
    "    # Gets words sorted by frequency\n",
    "    count.extend(collections.Counter(words).most_common())\n",
    "    dictionary = dict()\n",
    "\n",
    "    # Create an ID for each unique word\n",
    "    for word, c in count:\n",
    "        if c < 10:\n",
    "            continue\n",
    "        dictionary[word] = len(dictionary)\n",
    "        vocabulary_size += 1\n",
    "\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  \n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "\n",
    "    count[0][1] = unk_count\n",
    "\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "\n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10])\n",
    "print('Vocabulary size: ',vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация данных\n",
    "\n",
    "Здесь будут генерироваться данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, window_size):\n",
    "    global data_index \n",
    "    \n",
    "    # two numpy arras to hold target words (batch)\n",
    "    # and context words (labels)\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "\n",
    "    # span defines the total window size\n",
    "    span = 2 * window_size + 1 \n",
    "\n",
    "    # The buffer holds the data contained within the span\n",
    "    queue = collections.deque(maxlen=span)\n",
    "\n",
    "    # Fill the buffer and update the data_index\n",
    "    for _ in range(span):\n",
    "        queue.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "\n",
    "\n",
    "    for i in range(batch_size // (2*window_size)):\n",
    "        k=0\n",
    "        # Avoid the target word itself as a prediction\n",
    "        for j in list(range(window_size))+list(range(window_size+1,2*window_size+1)):\n",
    "            batch[i * (2*window_size) + k] = queue[window_size]\n",
    "            labels[i * (2*window_size) + k, 0] = queue[j]\n",
    "            k += 1 \n",
    "\n",
    "        # Everytime we read num_samples data points, update the queue\n",
    "        queue.append(data[data_index])\n",
    "\n",
    "        # If end is reached, circle back to the beginning\n",
    "        data_index = (data_index + np.random.randint(window_size)) % len(data)\n",
    "\n",
    "    return batch, labels\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "data_index = 0\n",
    "batch, labels = generate_batch(batch_size=8, window_size=2)\n",
    "print('\\nwith window_size = %d:' %2)\n",
    "print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Определение модели\n",
    "\n",
    "В модели будет содержаться следующее:\n",
    "\n",
    "* Гиперпараметры (например, batch_size)\n",
    "* Входной и выходной слои\n",
    "* Embedding-слой\n",
    "* Слой с нейронной сетью (softmax)\n",
    "* Loss\n",
    "* Loss Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 \n",
    "embedding_size = 64 \n",
    "window_size = 4 \n",
    "\n",
    "num_sampled = 32 # Number of negative examples to sample.\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Training input data (target word IDs).\n",
    "train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "# Training input label data (context word IDs)\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "################################################\n",
    "#            Model variables                   #\n",
    "################################################\n",
    "\n",
    "# Embedding layer\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "# Neural network weights and biases\n",
    "softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                        stddev=0.1 / math.sqrt(embedding_size))\n",
    ")\n",
    "softmax_biases = tf.Variable(tf.random_uniform([vocabulary_size],-0.01,0.01))\n",
    "\n",
    "# Look up embeddings for a batch of inputs.\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "#embed = tf.nn.dropout(embed, keep_prob=0.8)\n",
    "################################################\n",
    "#            Computes loss                     #\n",
    "################################################\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.sampled_softmax_loss(\n",
    "        weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "        labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size)\n",
    ")\n",
    "\n",
    "################################################\n",
    "#            Optimization                      #\n",
    "################################################\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "################################################\n",
    "#            For evaluation                    #\n",
    "################################################\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "normalized_embeddings = embeddings / norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 250001\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Initialize the variables in the graph\n",
    "tf.global_variables_initializer().run()\n",
    "print('Initialized')\n",
    "average_loss = 0\n",
    "\n",
    "for step in range(num_steps):\n",
    "\n",
    "    # Generate a single batch of data\n",
    "    batch_data, batch_labels = generate_batch(\n",
    "      batch_size, window_size)\n",
    "\n",
    "    # Optimize the embedding layer and neural network\n",
    "    # compute loss\n",
    "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "\n",
    "    # Update the average loss variable\n",
    "    average_loss += l\n",
    "\n",
    "    if (step+1) % 5000 == 0:\n",
    "        if step > 0:\n",
    "            average_loss = average_loss / 5000\n",
    "\n",
    "        print('Average loss at step %d: %f' % (step+1, average_loss))\n",
    "        average_loss = 0\n",
    "\n",
    "sg_embeddings = normalized_embeddings.eval()\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(perplexity=5, n_components=2, init='pca', n_iter=5000)\n",
    "\n",
    "# get the T-SNE manifold\n",
    "two_d_embeddings = tsne.fit_transform(sg_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_words = ['feline','cat','dog','canary','male','female','animal','pet','wolf','cats','dogs','wildcat','kittens']\n",
    "\n",
    "words = [reverse_dictionary[i] for i in np.arange(vocabulary_size)]\n",
    "\n",
    "pylab.figure(figsize=(15,15))\n",
    "# plot all the embeddings and their corresponding words\n",
    "for i, label in enumerate(words):\n",
    "    x, y = two_d_embeddings[i,:]\n",
    "    pylab.scatter(x, y, c='darkgray')   \n",
    "    if label in selected_words:\n",
    "        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
    "                       ha='right', va='bottom',fontsize=10)\n",
    "        \n",
    "pylab.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
